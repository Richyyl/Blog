---
title: Intro to Spark and building an AWS EMR cluster
author: Rich Louden
date: '2019-06-08'
slug: building-a-data-science-environment-with-aws-emr
categories: []
tags: []
type: ''
subtitle: ''
image: ''
editor_options: 
  chunk_output_type: console
---

This post has mainly been written for me, so that in the future I have a general reference guide for spark and so I don't have to piece together the various bits I've found strewn across the internet in order to build an EMR instance that I like. Also I originally wanted to build this to test whether reticulate could be used to link in with Spark on an EMR (elastic map reduce) cluster. However, I beleive that this is not possible now and as such had to pivot into utilising a jupyter notebook and rpy2 to allow me to utilise R within the session.

## Part 1: A quick introduction to spark

### What is spark?

Spark is a computing engine designed for large scale data computation, IE focused on cleaning and analyzing and not storage. It was developed at UC Berkeley and then donated to the Apache Software Federation, where it is maintained on an open source basis.

### How does it work?

Spark works through a number of servers joined together in a cluster, with one server acting as the master node in order to control the cluster and the others acting as executor nodes to distribute the data across (known as partitions) and perform actions, as shown below. 

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/clusters.jpg){:height=50% width=50%}

These actions are evaluated lazily, meaning that operations are not carried out until an action is required. For example multiplying a column will not occur until account is required of the output. This strategy allows spark to calculate and optimise a plan in which to run the operations across the cluster, increasing speed. 

In addition to being performed lazily, transformations can either be narrow or wide, which is explained below and also shown on the below diagram.

* Narrow transformation - Data in each partition can only be data in one partition, for example multiplying a column of numbers by a constant

* Wide transformation - Data in each partition can become data in multiple other partitions, for example sorting a partition.

The latter of these two is known as a shuffle, as it moves around the partitions, and involves writing the data to disk, where as narrow transformations are performed in memory, leading to differences in speed.

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/transformations.jpg){:height=50% width=50%}

### How is a cluster operated?

The majority of times a cluster will be operated via what is referred to as structured API, which acts as a bridge between a more common programming language and the native spark language. Currently these are available for scala, java, python SQL and R, where code written in these languages is translated within a spark session. A summation of the process is shown below, which involves a user propositioning the cluster manager, on the master node, for resources, leading to the creation of a spark driver process which includes the user code and a spark session. Once the full plan is determined, via the spark session, it is communicated to the cluster manager, which then allocates resources to run each process. The results of these processes are then fed back to the master node.

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/structured running.jpg){:height=50% width=50%}

## Part 2: Setting up an AWS EMR


### Step 1 - setting up the EMR cluster

The first step is finding your was to the emr homepage within AWS, which looks like below, where you can initiate a cluster from.

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/EMR home.jpg){ width=50% :height=50% width=50%
By clicking on create a cluster you'll find yourself being able to select your cluster options, including the name and software etc, choose spark and pick your key pair and you're good to launch. One additional thing you'll want to do is allow access to your S3 buckets, so that you can pull in data to use. A good guide for this is at the following [link](https://medium.com/@serkansakinmaz/how-to-connect-amazon-s3-via-emr-based-pyspark-42707d540881)

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/EMR config.jpg){:height=50% width=50%}

Once you've launched you can then set the security options, though only the first time you launch, these can be found on your cluster page post launch. Here you need to edit the inbound rules on both the master and slave nodes as open port 22 for your IP, via SSH, and a custom TCP for port 8888 which is open to all ips for jupyter notebooks.

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/EMR SG.jpg){:height=50% width=50%}
 
Once you've waiting the 10 or so minutes for the cluster to gear up you can ssh in, which needs to be conducted with port forwarding to allow your machine to connect to the master node and access the jupyter notebook via the browser. The command for this is shown below.

```{bash, eval=FALSE, echo=FALSE}

ssh -i your-authentication-key-pem-file -L 8888:localhost:8888 your-machine's-Public-DNS

ssh -i RL_EMR.pem -L 8888:localhost:8888 hadoop@ec2-18-130-34-143.eu-west-2.compute.amazonaws.com

```


Once you're in you, which may requiring entering a passcode you set earlier if you're using putty, should see the friendly screen below!

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/EMR login.jpg){:height=50% width=50%}


### Step 2 - Updating the EMR with required software

Now that we have our servers up and running and we've ssh'd into the main node it's time to update our software. For me this involves changing our default python on pyspark to python 3, installing libraries and setting jupyter to drive the spark processes.


Now we need to install some packages that we will likely need later on.
```{bash, eval=FALSE, echo=FALSE}

sudo pip-3.6 install jupyter
sudo pip-3.6 install pandas
sudo pip-3.6 install matplotlib
sudo pip-3.6 install seaborn

```


Now we need to change the default version of python used by spark, which is pretty easy going. This requires the following lines of code, which alters the python path in the spark config and sets the driver to our jupyter notebook.
```{bash, eval=FALSE, echo=FALSE}

export PYSPARK_PYTHON=/usr/bin/python3

export PYSPARK_DRIVER_PYTHON=jupyter

export PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port=8888'

source .bashrc

```


### Step 3 - Jupyter notebook

Now the infrastructure is all set up its time to open the notebook and login. This is done by accessing pyspark as below and copying the provided token. Once this is done you can start a new notebook which will be commected to your EMR cluster, as such you can utilise it analyse large datasets that are stored within your S3 buckets, providing you have set up the access already.


```{base, eval=FALSE, echo=FALSE}

pyspark

```

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/Spark jupyter.jpg){:height=50% width=50%}

![](content/post/2019-05-22-building-a-data-science-environment-with-aws-emr.en/Jupyter.jpg){:height=50% width=50%}
