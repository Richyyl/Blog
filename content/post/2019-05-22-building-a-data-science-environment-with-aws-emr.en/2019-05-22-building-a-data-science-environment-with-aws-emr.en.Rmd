---
title: Building a data science environment with AWS EMR
author: Rich Louden
date: '2019-05-22'
slug: building-a-data-science-environment-with-aws-emr
categories: []
tags: []
type: ''
subtitle: ''
image: ''
draft: True
editor_options: 
  chunk_output_type: console
---

This post has mainly been writen for me, so that in the future I dont have to piece together the various bits I've found strewn across the internet in order to build another EMR instance for when I'd like to use Spark. Also I originally wanted ot build this to test whether replicate could be used to link in with Spark on an EMR cluster.

##Step 1 - setting up the EMR cluster

The first step is finding your was to the emr homepage within AWS, which looks like below, where you can initiate a cluster from.

![alt text](EMR home.jpg)

By clicking on create a cluster youll find yourself being able to select your cluster options, including the name and software etc, choose spark and pick your key pair and you're good to launch.

![alt text](EMR config.jpg)

Once you've launched you can then set the security options, though only the first time you launch, these can be found on your cluster page post launch. Here you need to edit the inbound rules on both the master and slave nodes al open port 22 for your IP, via SSH, and a custom TCP for port 8787 which is open to all ips for rstudio server.

![alt text](EMR SG.jpg)
 
Once you've waitign the 10 or so minutes for the cluster to gear up you can ssh in, with from the terminal or putty, if you're on windows. This requires some of the details within the ssh box that you can expand from your clusters page, shown below.

![alt text](EMR putty.jpg)

Once you're in you need to add the passcode you applied to your keypair and then you should see the friendly screen below!

![alt text](EMR login.jpg)


##step 2 - Updating the EMR with required software

Now that we have our servers up and running and we've ssh'd into the main node it's time to update our software. For me this involves changing our default python to python 3, installing RStudio server and setting teh park python path to use the anaconda version I want to call through reticulate..


I found it best to first install rstudio server, using the following code, taking the file path from https://www.rstudio.com/products/rstudio/download-server/ and selecting the CentOS tab. At the same time we might as well create the user for said programme using the following code and open the ssh connection so that we can access it through the browser. The reason I do this first is it alters your path file and so would reset the anaconda path you set later on.
```{r}

wget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.1335-x86_64.rpm

yes | sudo yum install rstudio-server-rhel-1.2.1335-x86_64.rpm

sudo su
sudo useradd <username>
sudo echo <username>:<password> | chpasswd

```


Now we need to change the python path to python 3 and install some packages. I found the best way to do this was to install anaconda with python 3 and add it onto the path.
```{r}


yes | sudo yum update

su hadoop

wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh

bash Anaconda3-2019.03-Linux-x86_64.sh -b

PATH=~/anaconda3/bin:$PATH

which python


```


Now that we have the anaconda version set up as default we can change the spark version, which is pretty easy going, just requiring the following code, which alters the path in the spark config. Use the second command to check and the third to return back to the cluster home.
```{r}

sudo sed -i -e '$a\export PYSPARK_PYTHON=~/anaconda3/bin/python3' /etc/spark/conf/spark-env.sh

pyspark

exit()


```


Now the infrastructure is all set up its time to login and see if we can connect up reticulate etc.

```{r}
bash s3://eml-setup/aws-eml_v0.2.sh
```

