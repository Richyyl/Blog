---
title: The life of Sam Vimes - A Discworld NLP project
author: Rich Louden
date: '2018-04-28'
slug: the-life-of-sam-vimes-a-discworld-nlp-project
categories: []
tags: []
type: ''
subtitle: ''
image: ''
draft: True
---



<div id="the-life-of-sam-vimes-analysis" class="section level2">
<h2>the life of Sam Vimes analysis</h2>
<p>##check R bloggers the life-changing magic of tidying text and Rstudio pubs Game of thrones book analysis for ideas</p>
<p>install.packages(“pdftools”)
install.packages(“cleanNLP”)
install.packages(“readtext”)
library(plyr)
library(tidyverse)
library(pdftools)
library(RColorBrewer)
library(tidytext)
library(readtext)</p>
<div id="load-in-each-book-and-convert-to-data-frame" class="section level3">
<h3>load in each book and convert to data frame</h3>
<p>PDF_extraction &lt;- function(a){
pdf_text(a) %&gt;%
paste(.,collapse = &quot;“) %&gt;%
tibble(text = .) %&gt;%
unnest_tokens(sentence, text, token =”sentences&quot;)
}</p>
<p>WRD_extraction &lt;- function(x){
readtext(x) %&gt;%
unnest_tokens(sentence, text, “sentences”) %&gt;%
select(-1)
}</p>
<p>GG &lt;- WRD_extraction(“D08 - Guards! Guards!.doc”)</p>
<p>MAA &lt;- WRD_extraction(“Men at arms.docx”)</p>
<p>FOC &lt;- WRD_extraction(“Feet of clay.docx”)</p>
<p>J &lt;- WRD_extraction(“Jingo.docx”)</p>
<p>FE &lt;- WRD_extraction(“The Fifth Elephant.docx”)</p>
<p>NW &lt;- PDF_extraction(“Night Watch.pdf”)</p>
<p>THUD &lt;- PDF_extraction(“Pratchett_Terry-Discworld_34-Thud-Pratchett_Terry.pdf”)</p>
<p>SNUFF &lt;- PDF_extraction(“Snuff - Terry Pratchett.pdf”)</p>
<p>####clean up each book and then add to a single data frame for use</p>
<p>Clean_func &lt;- function(a,b,c){
a %&gt;%
slice(b) %&gt;%
mutate(Book = c,
Sentence_number = row_number())
}</p>
<p>GG_DF &lt;- Clean_func(GG, 8:10044, “Guards! Guards!”)
MAA_DF&lt;- Clean_func(MAA, 1:11113, “Men At Arms”)
FOC_DF&lt;- Clean_func(FOC, 2:10567, “Feet Of Clay”)
J_DF&lt;- Clean_func(J, 1:11360, “Jingo”)
FE_DF&lt;- Clean_func(FE, 2:11267, “Fifth Elephant”)
NW_DF&lt;- Clean_func(NW, 2:11619, “Night Watch”)
THUD_DF&lt;- Clean_func(THUD, 2:11195, “Thud”)
SNUFF_DF&lt;- Clean_func(SNUFF, 10:8072, “Snuff”)</p>
<p>Watch_levels &lt;- c(“Guards! Guards!”,
“Men At Arms”,
“Feet Of Clay”,
“Jingo”,
“Fifth Elephant”,
“Night Watch”,
“Thud”,
“Snuff”)</p>
<p>Watch_books &lt;- bind_rows(GG_DF, MAA_DF, FOC_DF, J_DF, FE_DF, NW_DF, THUD_DF, SNUFF_DF) %&gt;%
mutate(Book = factor(Book, levels = Watch_levels))</p>
<p>###perform sentiment analysis on sentences</p>
<p>Negative_sent &lt;- sentiments %&gt;%
filter(lexicon == “bing”, sentiment == “negative”)</p>
<p>Positive_sent &lt;- sentiments %&gt;%
filter(lexicon == “bing”, sentiment == “positive”)</p>
<p>Watch_books_words &lt;- Watch_books %&gt;%
unnest_tokens(word, sentence, token = “words”) %&gt;%
anti_join(stop_words) %&gt;%
group_by(Book, Sentence_number) %&gt;%
summarise(words = n())</p>
<p>watch_books_neg_sentiment &lt;- Watch_books %&gt;%
unnest_tokens(word, sentence, token = “words”) %&gt;%
anti_join(stop_words) %&gt;%
semi_join(Negative_sent)%&gt;%
group_by(Book, Sentence_number) %&gt;%
summarise(neg_words = n())</p>
<p>Watch_books_pos_sentiment &lt;- Watch_books %&gt;%
unnest_tokens(word, sentence, token = “words”) %&gt;%
anti_join(stop_words) %&gt;%
semi_join(Positive_sent) %&gt;%
group_by(Book, Sentence_number) %&gt;%
summarise(pos_words = n())</p>
<p>Watch_book_full_sentiment &lt;- Watch_books %&gt;%
left_join(watch_books_neg_sentiment, by = c(“Book”, “Sentence_number”)) %&gt;%
left_join(Watch_books_pos_sentiment,by = c(“Book”, “Sentence_number”)) %&gt;%
left_join(Watch_books_words,by = c(“Book”, “Sentence_number”)) %&gt;%
mutate(neg_words = recode(neg_words, ‘NA = 0’),
pos_words = recode(pos_words, ‘NA = 0’)) %&gt;%
filter(neg_words != 0 &amp; pos_words != 0) %&gt;%
mutate(neg_ratio = neg_words / words,
pos_ratio = pos_words / words,
sentiment = pos_ratio - neg_ratio) %&gt;%
select(-neg_words, pos_words, words)</p>
<p>nrc_sentimeent_plot &lt;- ggplot(data = Watch_book_full_sentiment, aes(x = Sentence_number, y = sentiment, fill = Book)) +
geom_bar(stat = “identity”, show.legend = F) +
scale_color_viridis_c() +
facet_wrap(~Book, ncol = 3, scales = “free_x”) +
theme_classic() +
labs(title = “Sentiment over the Night Watch history”, y = “sentiment”, x = “Sentence number”) +
theme(strip.text = element_text(face = “italic”)) +
theme(panel.grid.minor = element_blank())</p>
<p>nrc_sentimeent_plot</p>
<p>watch_books_AFINN &lt;- Watch_books %&gt;%
unnest_tokens(word, sentence, token = “words”) %&gt;%
inner_join(get_sentiments(“afinn”)) %&gt;%
group_by(Book, Sentence_number) %&gt;%
summarise(Sentiment_score = sum(score))</p>
<p>afinn_sentimeent_plot &lt;- ggplot(data = watch_books_AFINN, aes(x = Sentence_number, y = Sentiment_score, fill = Book)) +
geom_bar(stat = “identity”, show.legend = F) +
scale_color_viridis_c() +
facet_wrap(~Book, ncol = 3, scales = “free_x”) +
theme_minimal() +
labs(title = “Sentiment over the Night Watch history”, y = “sentiment”, x = “Sentence number”) +
theme(strip.text = element_text(face = “italic”)) +
theme(panel.grid.minor = element_blank())</p>
<p>afinn_sentimeent_plot</p>
<p>afinn_sent_summary &lt;- watch_books_AFINN %&gt;%
group_by(Book) %&gt;%
summarise(Overall_sentiment = sum(Sentiment_score))</p>
<p>afinn_summary_plot &lt;- ggplot(data = afinn_sent_summary, aes(x = Book, y = Overall_sentiment, fill= Book, group = 1)) +
geom_line(show.legend = F, col = “red”,lineend = “round”, linejoin = “round”, size = 1) +
geom_point(show.legend = F, col = “orange”) +
theme_classic() +
labs(title = “Overall sentiment score across the Night Watch books”, x = “Book”, y = “Overall sentiment score”) +
scale_color_brewer()</p>
<p>afinn_summary_plot</p>
<div id="bigrams-analysis" class="section level9">
<p>bigrams analysis</p>
<p>Watch_books_bigrams &lt;- Watch_books %&gt;%
unnest_tokens(bigram, sentence, token = “ngrams”, n = 2) %&gt;%
separate(bigram, c(“word1”, “word2”), sep = &quot; “) %&gt;%
filter(!word1 %in% stop_words<span class="math inline">\(word,  word1 != &quot;NA&quot;,  !word2 %in% stop_words\)</span>word,
word2 !=”NA“) %&gt;%
count(Book, word1, word2, sort = T) %&gt;%
unite(Bigram, word1, word2, sep =” &quot;) %&gt;%
slice(-1) %&gt;%
group_by(Book) %&gt;%
top_n(10) %&gt;%
arrange(Book, desc(n))</p>
<p>Bigram_plot &lt;- ggplot(data = Watch_books_bigrams, aes(x = reorder(Bigram,n), y = n, fill = Book)) +
geom_col(show.legend = F) +
coord_flip() +
facet_wrap(~Book, nrow = 3, scales = “free_y”) +
theme_minimal() +
labs(title = “Top ten bigrams in the Night Watch series”,
x = “Bigram”,
y = “Frequency”) +
scale_color_brewer() +
theme(strip.text = element_text(face = “italic”))
theme(panel.grid.minor = element_blank())</p>
<p>Bigram_plot</p>
<p>Gendered_bigrams &lt;- Watch_books %&gt;%
unnest_tokens(bigram, sentence, token = “ngrams”, n = 2) %&gt;%
separate(bigram, c(“word1”, “word2”), sep = &quot; “) %&gt;%
filter(word1 %in% c(”He“,”he“,”She“,”she“)) %&gt;%
count(Book, word1, word2, sort = T) %&gt;%
mutate(Grouping = case_when(word1 ==”He&quot; ~ “Male”,
word1 == “he” ~ “Male”,
word1 == “She” ~ “Female”,
word1 == “she” ~ “Female”),
Grouping = factor(Grouping)) %&gt;%
unite(Bigram, word1, word2, sep = &quot; &quot;) %&gt;%
group_by(Book) %&gt;%
top_n(10, n) %&gt;%
arrange(Book, desc(n))</p>
<p>Gendered_bigram_plot &lt;- ggplot(data = Gendered_bigrams, aes(x = reorder(Bigram,n), y = n, fill = Grouping)) +
geom_col() +
coord_flip() +
facet_wrap(~Book, nrow = 3, scales = “free_y”) +
theme_minimal() +
labs(title = “Top ten Gendered bigrams in the Night Watch series”,
x = “Bigram”,
y = “Frequency”) +
scale_color_brewer() +
theme(strip.text = element_text(face = “italic”)) +
theme(panel.grid.minor = element_blank())</p>
<p>Gendered_bigram_plot</p>
<p>#####dependency analaysis (GOT)</p>
<p>library(reticulate)
library(cleanNLP)</p>
<p>use_python(“/Library/Frameworks/Python.framework/Versions/3.7/bin/python3”)</p>
<p>cnlp_init_spacy()</p>
<p>setwd(“~/Desktop/Sam Vimes”)</p>
<p>PDF_extraction_2 &lt;- function(a){
pdf_text(a) %&gt;%
paste(.,collapse = &quot;&quot;)
}</p>
<p>WRD_extraction_2 &lt;- function(x){
readtext(x) %&gt;%
paste(.,collapse = &quot;&quot;)
}</p>
<p>GG_2 &lt;- WRD_extraction_2(“D08 - Guards! Guards!.doc”)</p>
<p>MAA_2 &lt;- WRD_extraction_2(“Men at arms.docx”)</p>
<p>J_2 &lt;- WRD_extraction_2(“Jingo.docx”)</p>
<p>GG_obj &lt;- cnlp_annotate(GG_2, as_strings = TRUE)</p>
<p>MAA_obj &lt;- cnlp_annotate(MAA_2, as_strings = TRUE)</p>
<p>J_obj &lt;- cnlp_annotate(J_2, as_strings = TRUE)</p>
<p>Entities &lt;- function(x){cnlp_get_entity(x) %&gt;%
filter(entity_type == “PERSON”) %&gt;%
group_by(entity) %&gt;%
count %&gt;%
ungroup()%&gt;%
arrange(desc(n))}</p>
<p>GG_people &lt;- Entities(GG_obj)</p>
<p>MAA_people &lt;- Entities(MAA_obj)</p>
<p>J_people &lt;- Entities(J_obj)</p>
<p>main_chars &lt;- c(“Carrot”, “Nobby”, “Angua”, “Sergeant Colon”, “Vimes”, “Lady Ramkin”, “Cuddy”, “Gaspode”)</p>
<p>dependencies_GG &lt;- cnlp_get_dependency(GG_obj, get_token = TRUE)</p>
<p>dependencies_MAA &lt;- cnlp_get_dependency(MAA_obj, get_token = TRUE)</p>
<p>dependencies_J &lt;- cnlp_get_dependency(J_obj, get_token = TRUE)</p>
<p>full_dep &lt;- bind_rows(dependencies_GG, dependencies_MAA, dependencies_J) %&gt;%
select(relation, word, word_target) %&gt;%
filter(word_target %in% main_chars &amp; relation == ‘nsubj’ &amp; word != “said”) %&gt;%
group_by(word_target, word, relation) %&gt;%
count %&gt;%
arrange(desc(n))</p>
<p>full_dep &lt;- read_csv(“RStudio/Scripts/Other projects/Life of Sam Vimes/dependencies_DF.csv”)</p>
<p>full_dep &lt;- dependencies_DF</p>
<p>watch_tfidf &lt;- bind_tf_idf(full_dep, word, word_target, n) %&gt;%
group_by(word_target) %&gt;%
top_n(10) %&gt;%
arrange(word_target)</p>
<p>watch_tfidf_plot &lt;- ggplot(data = watch_tfidf, aes( x = reorder(word, tf_idf), y = tf_idf, fill = word_target)) +
geom_col(show.legend = F) +
theme_minimal() +
coord_flip() +
facet_wrap(~word_target, nrow = 3, scales = “free_y”) +
labs(title = “TFIDF for charaters in Guards! Guards!, Men At Arms and Jingo!”, x = “Word”, y =“TFIDF”)+
theme(axis.text.y.left = element_text(size = 6))</p>
<p>watch_tfidf_plot</p>
<p>#####topic modelling (Youtube - Julia Silge)</p>
<p>install.packages(“stm”)
install.packages(“quanteda”)
library(stm)
library(quanteda)</p>
<p>Watch_dfm &lt;- Watch_books %&gt;%
unnest_tokens(word, sentence, token = “words”) %&gt;%
anti_join(stop_words) %&gt;%
count(Book, word, sort = T) %&gt;%
cast_dfm(Book, word, n)</p>
<p>processed_books &lt;- textProcessor(Watch_books$sentence, stem = F)</p>
<p>prepped_books &lt;- prepDocuments(processed_books<span class="math inline">\(documents, processed_books\)</span>vocab,lower.thresh = 5)</p>
<p>K_search&lt;-searchK(prepped_books<span class="math inline">\(documents,prepped_books\)</span>vocab,K=c(4:10))</p>
<p>plot(K_search)</p>
</div>
<div id="held-out-liklihood---better-models-show-increased-likihood-of-held-out-documents---ie-mor-elikely-to-hold-out-docs-in-training" class="section level5">
<h5>held out liklihood - better models show increased likihood of held out documents - ie mor elikely to hold out docs in training</h5>
</div>
<div id="residuals---lower-residuals-model-fits-better-to-the-data" class="section level5">
<h5>residuals - lower residuals = model fits better to the data</h5>
</div>
</div>
<div id="semantic-coherence---the-probability-of-words-ina-given-topic-co-occuring-together-used-to-measure-good-topic-quality.-the-higher-the-better" class="section level3">
<h3>semantic coherence - the probability of words ina given topic co-occuring together, used to measure good topic quality. The higher the better</h3>
</div>
<div id="lower-bound---an-estimate-of-the-lower-bound-of-that-distribution-used-to-estimate-parameters-higher-the-better" class="section level3">
<h3>lower bound - an estimate of the lower bound of that distribution, used to estimate parameters, higher the better</h3>
<p>###6K model based on results</p>
<p>Watch_books_stm &lt;- stm(Watch_dfm, K = 6, init.type = “Spectral”, verbose = F)</p>
<p>tidy_stm &lt;- tidy(Watch_books_stm)</p>
<p>top_beta &lt;- tidy_stm %&gt;%
group_by(topic) %&gt;%
top_n(10, beta) %&gt;%
ungroup() %&gt;%
mutate(topic = paste0(“Topic”, topic))</p>
<p>beta_plot &lt;- ggplot(data = top_beta, aes(x = term, y = beta, fill = as.factor(topic))) +
geom_col(show.legend = F) +
coord_flip() +
facet_wrap(~topic, scales = “free_y”, nrow = 2) +
labs(title = “Top ten terms per theme of the Night Watch books”, x = “Term”, y = “Beta”) +
theme_minimal()</p>
<p>beta_plot</p>
<p>Watch_gamma &lt;- tidy(Watch_books_stm, matrix = “gamma”,<br />
document_names = rownames(Watch_books_stm)) %&gt;%
mutate(topic = paste0(“Topic”, topic))</p>
<p>gamma_plot &lt;- ggplot(Watch_gamma, aes(gamma, fill = as.factor(topic))) +
geom_histogram(show.legend = FALSE) +
facet_wrap(~ topic, nrow = 2) +
theme_minimal() +
labs(title = “Document probabilities spread for each topic”,
y = “Number of books”, x = “Gamma”)</p>
<p>gamma_plot</p>
<p>topic_1_cloud &lt;- cloud(Watch_books_stm, topic = 2)</p>
</div>
</div>
